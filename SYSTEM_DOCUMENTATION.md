# zkRNN System Documentation

This document explains how the zkRNN repository is organised, how the prover/verifier pipeline works, and how to build, run, and extend the system. It is intended as the canonical reference for contributors who need to understand the whole stack—from quantised RNN execution through zero-knowledge proof generation, aggregation, and verification.

---

## 1. Repository Layout

- `src/` – C++ sources for the prover, verifier, polynomial commitment schemes, hash gadgets, and orchestration utilities.
- `tests/` – CMake-enabled unit and integration tests that exercise individual proof gadgets as well as the end-to-end streaming orchestrator (`tests/TESTING_GUIDE.md` lists scenarios).
- `circuits/` – Pre-compiled activation circuits (`*.pws`) used by the logup/GKR activation prover.
- `build/` – Generated artefacts after running `build.sh`, including compiled binaries, test executables, and benchmark outputs.
- `bench/` – Reference data dumps, e.g., `witness_dump.json` generated by instrumentation in the training pipeline.
- Shell scripts such as `build.sh`, `rnn_infer_Nova.sh`, `rnn_test_self.sh`, and `rnn_test_Nova.sh` wrap typical build/run flows.
- Documentation: this file, `IMPLEMENTATION_PLAN.md` (historical milestones), and `IMPLEMENTATION_ANALYSIS.md` (development notes).

---

## 2. Build and Execution Workflow

### 2.1 Prerequisites

- A Unix-like environment (or WSL on Windows) with `cmake`, `g++`, and standard build tools.
- The repository vendors KAIZEN dependencies (MiMC, logup, polynomial commitments, etc.) through the source tree; no external package manager is required.

### 2.2 Building Everything

Run the wrapper script from the repository root:

```
./build.sh
```

`build.sh` configures CMake, builds the main binary (`build/src/zkpot`), and compiles all tests under `build/tests/`.

### 2.3 Running the Prover Binary

`src/main.cpp` exposes a single command that drives both inference and training/PoGD proving. Usage is printed when arguments are missing:

```1676:1714:src/main.cpp
    if (argc < 7) {
        std::fprintf(stderr,
                     "Usage: %s T n m k pc_type arity [--mode=train|infer] [--model=path] [--inputs=path]\n",
                     argv[0]);
        return 1;
    }
...
    for (int i = 7; i < argc; ++i) {
        std::string arg(argv[i]);
        if (arg.rfind("--mode=", 0) == 0) {
            mode = arg.substr(7);
        } else if (arg.rfind("--model=", 0) == 0) {
            model_path = arg.substr(8);
        } else if (arg.rfind("--inputs=", 0) == 0) {
            input_path = arg.substr(9);
        } else if (arg.rfind("--snapshot=", 0) == 0) {
            snapshot_path = arg.substr(11);
        } else {
            std::fprintf(stderr, "Unknown argument: %s\n", arg.c_str());
            return 1;
        }
    }
```

- **Training mode (default)** performs full forward/backward/update cycles with proof generation, aggregation, and benchmarking.
- **Inference mode** (`--mode=infer`) forwards one batch and emits a proof of correct inference together with optional witness/model dumps.
- Optional flags load quantised models (`--model`) and inputs (`--inputs`), or replay a witness snapshot (`--snapshot`) captured previously.

To script typical runs, use the provided helpers:

- `./rnn_infer_Nova.sh <T> <input> <hidden> <output> [model_path] [input_path]`
- `./rnn_test_self.sh` to exercise a short self-test with synthetic data.

---

## 3. Core Concepts

### 3.1 Quantised RNN Execution

`RNN.cpp/.h` wraps KAIZEN’s fixed-point GRU/RNN routines. The struct `rnn_layer` holds weights, intermediate tensors, and gradient buffers for both forward (`rnn_layer::fwd`) and backward (`rnn_layer::bwd`) flows. Quantisation uses `quantization.cpp`, mapping floats onto `F` (alias of `virgo::fieldElement`) with `Q` fractional bits.

Forward execution (`rnn_forward`) computes:
1. `Wh = W_h * h_{t-1}`
2. `Wx = W_x * x_t`
3. `a_t = Wh + Wx + b1`
4. `h_t = tanh(a_t)` via lookup proofs
5. `z_t = W_y * h_t`
6. `yHat_t = softmax(z_t)`

Backward pass (`rnn_bptt`) accumulates gradients, and `rnn_update` applies quantised SGD, each accompanied by proof gadgets in `main.cpp`.

### 3.2 Field Arithmetic and Hashing

- `fieldElement.hpp/cpp` implement the finite field used throughout.
- `mimc.cpp/h` provide the MiMC hash used for transcripts and Merkle paths.
- `merkle_tree.cpp/h` and `mimc_sumcheck` helpers support Merkle-based commitments and hash consistency proofs.

### 3.3 Polynomial Commitments and Sumcheck

- `poly_commit.cpp/h` implements KAIZEN’s polynomial commitment scheme (`poly_commit`, `prove_verification`).
- Sumcheck generations (`generate_2product_sumcheck_proof`, etc.) and matrix-commit reductions live in `proof_utils.cpp`.
- `logup.cpp/.hpp` offers table lookup proofs used for exponentials inside activations.

---

## 4. Proof Generation Pipeline (Training Mode)

The top-level flow in `main.cpp` orchestrates data checks, RNN execution, and proof creation. At a high level:

1. **Dataset Integrity**: `check_input_rnn` hashes the public input sequence and generates MiMC sumcheck proofs to attest to data binding.
2. **Forward Pass Proof (`prove_rnn_forward`)**:
   - Proves logup lookups for tanh/softmax.
   - Uses `_prove_matrix2matrix` and `prove_matrix_add` to establish linear relations.
3. **Backward Pass Proof (`prove_rnn_backward`)**:
   - Covers gradient propagation via matrix multiplications (`prove_rnn_dWx`, `prove_rnn_dWh`, etc.).
   - Validates activation derivatives with `prove_tanh_backward`.
4. **Update Proof (`prove_rnn_update`)**:
   - Shows weights/biases update consistently with the learning rate.
5. **Aggregation (`prove_aggr`, `prove_aggr(...)`)**:
   - Commits witnesses through `poly_commit`.
   - Aggregates multiple rounds recursively, producing higher-level verification proofs.
6. **Benchmarking**:
   - `bench.hpp` utilities append timing/memory metrics to CSV logs (`bench_results.csv`, `bench_results_infer.csv`).

This entire loop repeats according to the configured arity and recursion depth, as shown in the second half of `main.cpp`.

---

## 5. Streaming IVC Components

The streaming interfaces allow step-wise proving over live inputs rather than full training epochs.

### 5.1 Leaf Proof (`prove_leaf.cpp`)

`ProveLeaf` produces the proof bundle for a single timestep:

- Uses `ProveSumMatMul` to batch the linear relations (`W_h·h_prev`, `W_x·x_t`, `W_y·h_t`) into a single block-diagonal MATMUL proof.
- Builds auxiliary witnesses (`BuildAuxWitness`) for fixed-point bit-decompositions.
- Invokes `ProveActivationGKR` to wrap KAIZEN’s GKR activation proofs (`activation_gkr.cpp`) alongside logup lookups.
- Packs results into `LeafResult`, exposing raw transcript entries and logup proofs while keeping an optional `step_proof` for legacy fallbacks.

```139:205:src/prove_leaf.cpp
  vector<vector<FieldVector>> matrices_combined{weights.W_h, weights.W_x, weights.W_y};
  vector<FieldVector> vectors_combined{in.h_prev, in.x_t, out.h_t};
  FieldVector combined_result;
  combined_result.reserve(wh_result.size() + wx_result.size() + zy_result.size());
  combined_result.insert(combined_result.end(), wh_result.begin(), wh_result.end());
  combined_result.insert(combined_result.end(), wx_result.begin(), wx_result.end());
  combined_result.insert(combined_result.end(), zy_result.begin(), zy_result.end());

  SumMatMulResult linear_proof = ProveSumMatMul(matrices_combined, vectors_combined, combined_result);

  ActivationProofs tanh_proofs = ProveActivationGKR(masked_a, aux_a, out.h_t,
                                                    out.exp_tanh,
                                                    ACTIVATION_TANH, num_bits);
```

### 5.2 SumMatMul Wrapper (`sum_matmul_wrapper.cpp`)

Wraps `_prove_matrix2matrix` to produce canonical matmul proofs, padding matrices and generating evaluation randomness. Handles both single and combined products, enabling SUMMER-style batching.

### 5.3 Activation GKR (`activation_gkr.cpp`)

Bridges auxiliary witness consistency (`ProveAuxConsistency`) with logup table proofs for tanh/softmax. It validates lookup table queries from `ExpBatch` emitted during forward passes.

### 5.4 Incremental Verifiable Computation Adapter (`ivc_adapter.cpp`)

`FA_Aggregate` folds a collection of leaf proofs plus the previous accumulator into a new recursive proof by:

1. Normalising proof structures for encoding.
2. Building a verification bundle (`BuildVerificationBundle`) that captures all sumcheck/GKR transcripts.
3. Running `prove_verification` to obtain a new accumulator proof and serialising it.

```73:162:src/ivc_adapter.cpp
  std::cout << "[FA_Aggregate] folding " << children.size()
            << " leaves with prev_acc=" << prev_acc << "\n";
  ...
  struct proof acc_proof = prove_verification(gkr_data, randomness, bundle.transcript);
  std::string accumulator = SerializeProofToString(acc_proof);
  result.serialized = accumulator;
  result.proof_struct = acc_proof;
  result.ok = true;
```

### 5.5 Stream Orchestrator (`stream_orchestrator.cpp`)

Maintains a rolling accumulator across inputs:

1. Quantises the incoming timestep (`TimeStepInput`) and ensures weights are initialised.
2. Runs a single-timestep forward pass to derive outputs and logup batches.
3. Calls `ProveLeaf` and buffers the result.
4. Once `arity_k` leaves are available, calls `FA_Aggregate` and updates the accumulator proof.
5. `FinaliseAndOpen` packages the accumulator with `FinaliseProof` and runs `VerifyProof`.

---

## 6. Finalisation and Verification

- `finalise_proof.cpp` serialises the last accumulator into a `FinalProof`, allowing callers to supply an already-decoded proof or a folded accumulator string.
- `verifier_stub.cpp` resets the MiMC transcript, deserialises proofs if needed, and dispatches to concrete verifiers (`verify_matrix2matrix`, `verify_gkr`, `verify_bit_decomposition`) based on `proof.type`.

These utilities are used both by the streaming orchestrator and by top-level flows in `main.cpp`.

---

## 7. Witness and Snapshot Handling

- `witness_snapshot.cpp/h` support dumping training state (`DumpTrainingSnapshot`) to JSON. Snapshots capture model weights, intermediate tensors, and activation lookup indices so that inference proofs can be replayed deterministically.
- `get_witness_rnn`/`get_model_rnn` (in `main.cpp`) flatten witnesses for polynomial commitments.
- The helper respects `BENCH_DUMP`/`BENCH_CSV` environment variables to choose output locations.

---

## 8. Testing Strategy

The project includes staged tests (see `tests/TESTING_GUIDE.md`):

1. **Unit-level gadgets**: `test_aux_witness`, `test_sum_matmul_wrapper`, `test_activation_gkr`.
2. **Integration**: `test_prove_leaf`, `test_ivc_adapter`.
3. **System-level**: `test_finalise_verifier`, `test_stream_orchestrator`.

Run `./tests/run_all_tests.sh` or invoke individual binaries in `build/tests/`. Tests initialise shared globals (`init_hash`, `init_SHA`) to mirror the prover environment.

---

## 9. Benchmarks and Instrumentation

During training runs, the prover records:

- Proof sizes (`proof_size`),
- Prover/verifier timing,
- Aggregation/commitment overhead,
- Peak resident memory (`PeakMemGB`).

Logs append to CSVs under `build/src/bench_results*.csv` for post-analysis.

---

## 10. Extending the System

- **Adding activations**: implement circuit loaders in `activation_circuit_loader.*`, provide logup tables, and update `ActivationType`.
- **Alternative commitments**: adapt `poly_commit.cpp` and `config_pc.hpp` to target different curve/field settings.
- **Custom orchestrators**: reuse `ProveLeaf`, `FA_Aggregate`, `FinaliseProof`, and `VerifyProof` to build bespoke pipelines.
- **Snapshot tooling**: extend `WitnessSnapshot` schema to include additional tensors or metadata required for reproducibility.

---

## 11. Quick Reference

- Build: `./build.sh`
- Inference proof: `./rnn_infer_Nova.sh T n m k [model inputs]`
- Streaming orchestrator entry: `StreamOrchestrator::OnInput`
- Leaf proof gadget: `ProveLeaf`
- Aggregation: `FA_Aggregate`
- Finalise & verify: `FinaliseProof`, `VerifyProof`
- Tests: `./tests/run_all_tests.sh`

---

For historical context and detailed implementation notes, consult `IMPLEMENTATION_ANALYSIS.md`. For planning milestones, see `IMPLEMENTATION_PLAN.md`.


